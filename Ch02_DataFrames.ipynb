{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch02_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JdIfR70Jxu2"
   },
   "source": [
    "### Set up the Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "os_Mz8CUJxu5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/home/student/ROI/Spark/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vl7-cmUJxu-"
   },
   "source": [
    "### Turn a simple RDD into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sou-X5kCJxu_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
    "x0 = spark.createDataFrame(x)\n",
    "x0.show()\n",
    "print(x0.collect())\n",
    "list(map(tuple, x0.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_1Cg_xuJxvE"
   },
   "source": [
    "### Give the DataFrame meaningful column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udSoOBO5JxvF"
   },
   "outputs": [],
   "source": [
    "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
    "#x1 = spark.createDataFrame(x, schema='ID, Name') # Does not work\n",
    "x1.show()\n",
    "print(x1)\n",
    "print(x1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiGdbXBVJxvJ"
   },
   "source": [
    "### Give a DataFrame a schema with column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en8OdGGlJxvK"
   },
   "outputs": [],
   "source": [
    "x2 = spark.createDataFrame(x, schema='ID:int, Name:string')\n",
    "x2.show()\n",
    "print(x2)\n",
    "\n",
    "x3 = x2.rdd.map(lambda x : (x.ID * 10, x.Name.upper()))\n",
    "\n",
    "x4 = spark.createDataFrame(x3)\n",
    "x4.show()\n",
    "\n",
    "x5 = x3.toDF(schema='ID:int, Name:string')\n",
    "x5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could also use a structured type object to specify schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('ID', IntegerType()), \n",
    "    StructField('Name', StringType())\n",
    "])\n",
    "x6 = spark.createDataFrame(x, schema=schema)\n",
    "display(x6)\n",
    "\n",
    "x7 = x.toDF(schema=schema)\n",
    "display(x7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlhoOJwZJxvO"
   },
   "source": [
    "### Load a text file into a RDD and clean it up as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxFb9sk4JxvQ"
   },
   "outputs": [],
   "source": [
    "filename = f'{datapath}/finance/CreditCard.csv'\n",
    "cc = sc.textFile(filename)\n",
    "first = cc.first()\n",
    "cc = cc.filter(lambda x : x != first)\n",
    "cc.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6aNyMRpJxvV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "cc = cc.map(lambda x : x.split(',')) \n",
    "cc.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8w-d27x5JxvZ"
   },
   "outputs": [],
   "source": [
    "cc = cc.map(lambda x : (x[0][1:], x[1][1:-1], datetime.datetime.strptime(x[2], '%d-%b-%y').date(), x[3], x[4], x[5], float(x[6])))\n",
    "print (cc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mk7f9YAWJxvc"
   },
   "source": [
    "### Turn the RDD into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2iXnrkJJxvu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = cc.toDF('City: string, Country: string, Date: date, CardType: string, TranType: string, Gender: string, Amount: double')\n",
    "df.show()\n",
    "print(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better yet, just use the built in CSV reader to read a file directly into a DataFrame and skip RDD's altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipperfile = f'{datapath}/northwind/CSV/shippers'\n",
    "df = spark.read.csv(shipperfilen, header=True, inferSchema=True)\n",
    "# this doesn't work though\n",
    "# df = spark.read.csv(filename, schema='ShipperID:string, CompanyName:string, Phone:string')\n",
    "display(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To read a CSV file with a specific schema you must use the StructType, you can't just pass in a string with column names and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('ShipperID', IntegerType()), \n",
    "    StructField('CompanyName', StringType()), \n",
    "    StructField('Phone', StringType())\n",
    "])\n",
    "shippers = spark.read.csv(shipperfile, schema = schema, header=False)\n",
    "shippers.printSchema()\n",
    "display(shippers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdAF7yVmJxvy"
   },
   "source": [
    "### ***LAB:*** Read regions from the the HDFS folder using read.csv and territories from the /home/student/ROI/Spark/datasets/northwind/CSVHeaders folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIbiLKz0Jxvz"
   },
   "outputs": [],
   "source": [
    "territoriesfile= f'{datapath}/northwind/CSVHeaders/territories'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqpW0HPKJxv2"
   },
   "source": [
    "### Convert a DataFrame into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I64RwvUdJxv3"
   },
   "outputs": [],
   "source": [
    "print (shippers.toJSON().take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2PVAJGSJxv-"
   },
   "source": [
    "### Choose particular columns from a DataFrame and create a calculated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shp65QEQJxv_"
   },
   "outputs": [],
   "source": [
    "display(df.select('City', df.Date, df.Amount) \\\n",
    "          .withColumn('BigAmount', df.Amount * 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a bunch of different transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOXLR2IPJxwD"
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "# display(df.select('City', 'Date', 'Card Type', 'Exp Type', 'Amount'))\n",
    "# display(df.drop('Gender'))\n",
    "# display(df.withColumnRenamed('Card Type', 'CardType'))\n",
    "# display(df.select('City').distinct())\n",
    "# print (df.count(), df.select('City').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9NySqdqJxwH"
   },
   "source": [
    "### Sort a DataFrame. The sort and orderBy methods are different aliases for the exact same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9IkEP-4JxwI"
   },
   "outputs": [],
   "source": [
    "df.sort(df.Amount).show()\n",
    "df.sort(df.Amount, ascending = False).show()\n",
    "df.select('City', 'Amount').orderBy(df.City).show()\n",
    "df.withColumn('NewAmount', df.Amount * 10).where('NewAmount >= 1000').where('Amount <= 2000')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6zM9-5MJxwS"
   },
   "source": [
    "### Create a new DataFrame with a new calculated column added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfXZYr82JxwU"
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('Discount', df.Amount * .03) \\\n",
    "        .withColumnRenamed('Card Type', 'CardType') \\\n",
    "        .withColumnRenamed('Exp Type', 'ExpType')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsHhY9u9Jxwh"
   },
   "source": [
    "### The filter and where methods can both be used and have alternative ways to represent the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bg7uhoEJxwj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df2.filter(df2.Amount < 4000))\n",
    "print(df2.filter('Amount < 4000').count())\n",
    "print(df2.where('Amount < 4000').count())\n",
    "print(df2.where(df2.Amount < 4000).count())\n",
    "\n",
    "print (df2.where((df2.Amount >= 3000) & (df2.Amount <= 4000)).count())\n",
    "print (df2.where('Amount >= 3000 AND Amount <= 4000').count())\n",
    "print (df2.where('Amount BETWEEN 3000 AND 4000').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5E4afFTJxwp"
   },
   "source": [
    "### ***LAB:*** Using the df2 DataFrame, answer the following questions:\n",
    "\n",
    "\n",
    "*   How many Platinum card purchases were there with a discount above $100?\n",
    "*   Find the ten biggest discount amounts earned by women and show just the purchase amount, discount, and date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kZwJv9LJxwr"
   },
   "outputs": [],
   "source": [
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRYHW5DlJxwx"
   },
   "source": [
    "### JOINs work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLGYJT1PJxwy"
   },
   "outputs": [],
   "source": [
    "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
    "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('childID:int, name:string, parentID:int')\n",
    "display(tab1)\n",
    "display(tab2)\n",
    "display(tab1.join(tab2, tab1.ID == tab2.parentID))\n",
    "display(tab1.join(tab2, tab1.ID == tab2.parentID, 'left'))\n",
    "display(tab1.join(tab2, tab1.ID == tab2.parentID, 'right'))\n",
    "display(tab1.join(tab2, tab1.ID == tab2.parentID, 'full'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbKSw8mFJxw1"
   },
   "source": [
    "### Examples of aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q-LPpNaJxw2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
    "display(tab3)\n",
    "tab3.groupby('groupID').max().show()\n",
    "tab3.groupby('groupID').sum().show()\n",
    "x = tab3.groupby('groupID')\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "#from pyspark.sql.functions import sum, max\n",
    "# print(dir(F))\n",
    "x.agg(F.sum('amount'), F.max('amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5J3KIQfJxw5"
   },
   "source": [
    "### Examples of reading a CSV directly into a DataFrame using different styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJWVKyXeJxw6"
   },
   "outputs": [],
   "source": [
    "filename = f'{datapath}/finance/CreditCard.csv'\n",
    "df4 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmIrAg1KJxxA"
   },
   "outputs": [],
   "source": [
    "df4 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQVzZUyVJxxE"
   },
   "outputs": [],
   "source": [
    "df4 = spark.read.csv(filename, header = True, inferSchema = True)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlpJkJKkJxxI"
   },
   "outputs": [],
   "source": [
    "display(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pxxBU4AJxxS"
   },
   "source": [
    "### Another way of changing column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RUqQyieJxxU"
   },
   "outputs": [],
   "source": [
    "cols = df4.columns\n",
    "cols[0] = 'CityCountry'\n",
    "df4 = df4.toDF(*cols)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDGFU2bwJxxN"
   },
   "source": [
    "### ***LAB:*** Read the Products file from the JSON folder and categories from the CSVHeaders folder, then join them displaying just the product and category IDs and names, and sort by categoryID then productID. \n",
    "\n",
    "\n",
    "**Hint:** Drop the ambiguous column after the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTxpSZd7JxxP",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk-WlDwNJxxW"
   },
   "source": [
    "### Apply a custom UDF to columns to separate the City and Country and convert the Date into a date datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dcWXI1iJxxX"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, expr\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date\n",
    "import datetime\n",
    "\n",
    "def city(x):\n",
    "    return x[:x.find(',')]\n",
    "\n",
    "cityUDF = udf(city, StringType())\n",
    "\n",
    "def country(x):\n",
    "    return x[x.find(',') + 1 :]\n",
    "\n",
    "@udf(StringType())\n",
    "def gender(x):\n",
    "    return 'Male' if x == 'M' else 'Female'\n",
    "\n",
    "df5 = df4.withColumn('City', cityUDF(df4.CityCountry)) \\\n",
    "      .withColumn('Country', udf(country, StringType())(df4.CityCountry)) \\\n",
    "      .withColumn('Date', to_date(df4.Date, 'dd-MMM-yy')) \\\n",
    "      .withColumn('Gender', gender(df4.Gender)) \\\n",
    "      .drop(df4.CityCountry)\n",
    "display(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJShtBBYJxxe"
   },
   "source": [
    "### DataFrames can be written to a variety of file formats. Here we are writing it to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lds2CsoKJxxf"
   },
   "outputs": [],
   "source": [
    "df5.write.mode('overwrite').json(f'{rootpath}/CreditCard.json')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1hZdPH5Jxxm"
   },
   "source": [
    "### Read a JSON file into a DataFrame, but note that we lose the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYzveIUNJxxn"
   },
   "outputs": [],
   "source": [
    "df6 = spark.read.json(f'{rootpath}/CreditCard.json')\n",
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex6IIfYYJxxp"
   },
   "source": [
    "### Create a schema that can be used to import a file and directly name the columns and convert them to the desired data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEYmzqk8Jxxq"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('Date', DateType()), \n",
    "    StructField('Card Type', StringType()),\n",
    "    StructField('Exp Type', StringType()),\n",
    "    StructField('Gender', StringType()),\n",
    "    StructField('Amount', FloatType()),\n",
    "    StructField('City', StringType()),\n",
    "    StructField('Country', StringType())\n",
    "])\n",
    "df6 = spark.read.json(f'{rootpath}/CreditCard.json', schema = schema)\n",
    "df6.printSchema()\n",
    "display(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Ch03_DataFrames.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
