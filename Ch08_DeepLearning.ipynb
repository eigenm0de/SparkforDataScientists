{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Ch08_DeepLearning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataScientists/blob/Development/Ch08_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlXnSrvwP30l",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning With Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQTF1sWRP30q",
        "colab_type": "text"
      },
      "source": [
        "### Load Spark-DL into the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGYuVqMeP30t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install tensorflow==1.15\n",
        "import os\n",
        "SUBMIT_ARGS = \"--packages databricks:spark-deep-learning:1.0.0-spark2.3-s_2.11 pyspark-shell\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DL with Spark Deep Cognition\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk7DNY7uP30y",
        "colab_type": "text"
      },
      "source": [
        "### Load an image.\n",
        "\n",
        "Let's do a sample image recognition using Deep Learning, but loading some images into Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Du9EFrcP301",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\n",
        "# !tar xzf flower_photos.tgz\n",
        "# ! rm flower_photos.tgz\n",
        "# !mkdir flower_photos/sample\n",
        "# !cp flower_photos/daisy/100080576_f52e8ee070_n.jpg flower_photos/sample/\n",
        "# !cp flower_photos/daisy/10140303196_b88d3d6cec.jpg flower_photos/sample/\n",
        "# !cp flower_photos/tulips/100930342_92e8746431_n.jpg flower_photos/sample/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDSSSplcP307",
        "colab_type": "text"
      },
      "source": [
        "### Display a few images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "iEuvUTQhP308",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display_png, Image, display\n",
        "import os\n",
        "\n",
        "path = '/home/student/ROI/Spark/flower_photos/'\n",
        "files = os.listdir(path + 'sample/')\n",
        "images = [Image(filename = path + 'sample/' + file, format = 'png') for file in files if file.endswith('.jpg')]\n",
        "for ea in images:\n",
        "    display_png(ea)\n",
        "\n",
        "print ('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANTvtsUjP31A",
        "colab_type": "text"
      },
      "source": [
        "### Read images into a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9r9v1fLP31B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.image import ImageSchema\n",
        "image_df = ImageSchema.readImages(path + \"sample/\")\n",
        "image_df.printSchema()\n",
        "#image_df.take(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwQo1YZgP31H",
        "colab_type": "text"
      },
      "source": [
        "### We need to install TensorFlow and Keras, but for this demo we need an older version of TensorFlow.\n",
        "We split up the dataets as we normally would into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCmujF61P31J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow==1.15\n",
        "# !pip install keras\n",
        "\n",
        "from pyspark.ml.image import ImageSchema\n",
        "from pyspark.sql.functions import lit\n",
        "from sparkdl.image import imageIO\n",
        "\n",
        "tulips_df = ImageSchema.readImages(path + \"tulips/\").withColumn(\"label\", lit(1))\n",
        "daisy_df = imageIO.readImagesWithCustomFn(path + \"daisy/\", decode_f=imageIO.PIL_decode).withColumn(\"label\", lit(0))\n",
        "\n",
        "tulips_train, tulips_test, _ = tulips_df.randomSplit([0.1, 0.05, 0.85])  # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
        "daisy_train, daisy_test, _ = daisy_df.randomSplit([0.1, 0.05, 0.85])     # use larger training sets (e.g. [0.6, 0.4] for getting more images)\n",
        "\n",
        "train_df = tulips_train.unionAll(daisy_train)\n",
        "test_df = tulips_test.unionAll(daisy_test)\n",
        "\n",
        "display(train_df)\n",
        "#print(train_df.take(1))\n",
        "\n",
        "# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
        "# This ensure that each of the paritions has a small size.\n",
        "#train_df = train_df.repartition(100)\n",
        "#test_df = test_df.repartition(100)\n",
        "print ('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXcghWWpP31O",
        "colab_type": "text"
      },
      "source": [
        "### Let’s train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SseK9lMzP31P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nano /etc/java-11-openjdk/accessibility.properties\n",
        "# comment out assistive_technologies=org.GNOME.Accessibility.AtkWrapper\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from sparkdl import DeepImageFeaturizer \n",
        "\n",
        "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
        "p = Pipeline(stages=[featurizer, lr])\n",
        "\n",
        "p_model = p.fit(train_df)\n",
        "print ('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YAz-v-CP31T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#p_model.save('/home/student/ROI/flower.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "yMSSNEjkP31a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyspark.ml import PipelineModel\n",
        "# p_model2 = PipelineModel.load('/home/student/ROI/flower.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TINUxs01P31e",
        "colab_type": "text"
      },
      "source": [
        "### Look at how well the model performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K49fvsRP31f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = p_model.transform(test_df)\n",
        "predictions.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyY_qQa7P31i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "# evaluator = MultiClassificationEvaluator()\n",
        "# evaluator.evaluate(predictions\n",
        "x = predictions.select(\"prediction\", \"label\").withColumn(\"label\", predictions[\"label\"].cast(DoubleType()))\n",
        "#print(x.take(10))\n",
        "metrics = MulticlassMetrics(x.rdd)\n",
        "print(metrics.confusionMatrix())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9zkyQR1P31q",
        "colab_type": "text"
      },
      "source": [
        "### We can take look at where we are making mistakes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qpQy2SvP31r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def _p1(v):\n",
        "    return float(v.array[1])\n",
        "take_one = udf(_p1, DoubleType())\n",
        "\n",
        "df = predictions.withColumn(\"p\", take_one(predictions.probability))\n",
        "wrong_df = df.orderBy(expr(\"abs(p - label)\"), ascending=False)\n",
        "#wrong_df.select(\"image.origin\", \"p\", \"label\").show(10)\n",
        "wrong_df.createOrReplaceTempView('wrong')\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqIqOWFGP31u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = spark.sql('select image.origin as origin, p, label from wrong order by p desc').collect()\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyX68Z0YP31y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(x)\n",
        "for label in range(2):\n",
        "    for file in [f for f in x if f.label == label][:3]:\n",
        "        print (file.origin[5:])\n",
        "        display_png(Image(filename = file.origin[5:], format = 'png') )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcnvlC-gP315",
        "colab_type": "text"
      },
      "source": [
        "### Working with general tensors\n",
        "\n",
        "Deep Learning Pipelines also provides ways to apply models with tensor inputs\n",
        "(up to 2 dimensions), written in popular deep learning libraries:\n",
        "\n",
        "* TensorFlow graphs\n",
        "* Keras models\n",
        "\n",
        "In this article, we will focus only on the Keras models. The `KerasTransformer`\n",
        "applies a TensorFlow-backed Keras model to tensor inputs of up to 2 dimensions.\n",
        "It loads a Keras model from a given model file path and applies the model to a\n",
        "column of arrays (where an array corresponds to a Tensor), outputting a column\n",
        "of arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu1dxDrHP316",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from sparkdl import KerasTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Generate random input data\n",
        "num_features = 10\n",
        "num_examples = 100\n",
        "input_data = [{\"features\" : np.random.randn(num_features).astype(float).tolist()} for i in range(num_examples)]\n",
        "schema = StructType([ StructField(\"features\", ArrayType(FloatType()), True)])\n",
        "input_df = spark.createDataFrame(input_data, schema)\n",
        "\n",
        "# Create and save a single-hidden-layer Keras model for binary classification\n",
        "# NOTE: In a typical workflow, we'd train the model before exporting it to disk,\n",
        "# but we skip that step here for brevity\n",
        "model = Sequential()\n",
        "model.add(Dense(units=20, input_shape=[num_features], activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model_path = \"simple-binary-classification\"\n",
        "model.save(model_path)\n",
        "\n",
        "# Create transformer and apply it to our input data\n",
        "transformer = KerasTransformer(inputCol=\"features\", outputCol=\"predictions\", modelFile=model_path)\n",
        "final_df = transformer.transform(input_df)\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHBU1T0P31-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjtPHUCOP32B",
        "colab_type": "text"
      },
      "source": [
        "### Deploying Models in SQL\n",
        "\n",
        "One way to productionize a model is to deploy it as a Spark SQL User Defined Function, which allows anyone who knows SQL to use it. Deep Learning Pipelines provides mechanisms to take a deep learning model and register a Spark SQL User Defined Function (UDF). In particular, Deep Learning Pipelines 0.2.0 adds support for creating SQL UDFs from Keras models that work on image data.\n",
        "\n",
        "The resulting UDF takes a column (formatted as a image struct “SpImage”) and produces the output of the given Keras model; e.g. for Inception V3, it produces a real valued score vector over the ImageNet object categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjtoi3xVP32D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import InceptionV3\n",
        "from sparkdl.udf.keras_image_model import registerKerasImageUDF\n",
        "\n",
        "registerKerasImageUDF(\"inceptionV3_udf\", InceptionV3(weights=\"imagenet\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTPiTLSiP32J",
        "colab_type": "text"
      },
      "source": [
        "In Keras workflows dealing with images, it’s common to have preprocessing steps before the model is applied to the image. If our workflow requires preprocessing, we can optionally provide a preprocessing function to UDF registration. The preprocessor should take in a filepath and return an image array; below is a simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENtuAIOlP32K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import InceptionV3\n",
        "from sparkdl.udf.keras_image_model import registerKerasImageUDF\n",
        "\n",
        "def keras_load_img(fpath):\n",
        "    from keras.preprocessing.image import load_img, img_to_array\n",
        "    import numpy as np\n",
        "    img = load_img(fpath, target_size=(299, 299))\n",
        "    return img_to_array(img).astype(np.uint8)\n",
        "\n",
        "registerKerasImageUDF(\"inceptionV3_udf_with_preprocessing\", InceptionV3(weights=\"imagenet\"), keras_load_img)\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl7qxbsEP32S",
        "colab_type": "text"
      },
      "source": [
        "Once a UDF has been registered, it can be used in a SQL query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgJWDC0xP32Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.image import ImageSchema\n",
        "\n",
        "image_df = ImageSchema.readImages(\"flower_photos/sample/\")\n",
        "image_df.registerTempTable(\"sample_images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyu8pwYNP32f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(image_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "q4ZrOOjRP32i",
        "colab_type": "text"
      },
      "source": [
        "### Once a data scientist builds the desired model, Deep Learning Pipelines makes it simple to expose it as a function in SQL, so anyone in their organization can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpWvps0hP32l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(spark.sql(\"SELECT inceptionV3_udf_with_preprocessing(image) as predictions from sample_images\"))\n",
        "#.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEWDLSvAP32o",
        "colab_type": "text"
      },
      "source": [
        "## Code below does not work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiuKBiu8P32p",
        "colab_type": "text"
      },
      "source": [
        "### Spark prediction pipeline using InceptionV3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP6doYbjP32q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparkdl import DeepImagePredictor\n",
        "# Read images using Spark\n",
        "image_df = ImageSchema.readImages(\"flower_photos/sample/\")\n",
        "predictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\n",
        "predictions_df = predictor.transform(image_df)\n",
        "print ('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRY_VEq3P32t",
        "colab_type": "text"
      },
      "source": [
        "### Let's take a look to the predictions dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AVI0XKaP32u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_df.select(\"predicted_labels\").show(truncate=False,n=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB20XeEXP32y",
        "colab_type": "text"
      },
      "source": [
        "### Notice that the 'predicted_labels' column shows 'daisy' as a high probability class for all of sample flowers using this base model, for some reason the tulip was closer to a picket fence than to a flower (maybe because of the background of the photo). \n",
        "\n",
        "However, as can be seen from the differences in the probability values, the\n",
        "neural network has the information to discern the two flower types. Hence our\n",
        "transfer learning example above was able to properly learn the differences\n",
        "between daisies and tulips starting from the base model.\n",
        "\n",
        "Let’s see how well our model discern the type of the flower:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnx0PfGWP320",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = p_model.transform(image_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LISxWjvKP325",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iR4_6hTP32_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _p1(v):\n",
        "    return float(v.array[1])\n",
        "take_one = udf(_p1, DoubleType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOWkpIl_P33F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 100930342_92e8746431_n.jpg not a daisy\n",
        "df.select(\"image.origin\",(1-take_one(df.probability)).alias(\"p_daisy\")).show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AkpQPTVUP33J",
        "colab_type": "text"
      },
      "source": [
        "## For Keras \n",
        "\n",
        "To use the transformer, we first need to have a Keras model stored as a file. For this notebook we'll just save the Keras built-in InceptionV3 model instead of training one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McrLXXHrP33K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import InceptionV3\n",
        "model = InceptionV3(weights=\"imagenet\")\n",
        "model.save('model-full.h5')\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qnk8zfsP33O",
        "colab_type": "text"
      },
      "source": [
        "### Now we will create a Keras transformer but first we will preprocess the images to work with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gACeHZmP33P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "from pyspark.sql.types import StringType\n",
        "from sparkdl import KerasImageFileTransformer\n",
        "\n",
        "def loadAndPreprocessKerasInceptionV3(uri):\n",
        "  # this is a typical way to load and prep images in keras\n",
        "    image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    return preprocess_input(image)\n",
        "\n",
        "transformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n",
        "                                        modelFile='model-full.h5',  # local file path for model\n",
        "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n",
        "                                        outputMode=\"vector\")\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcYz5RTHP33S",
        "colab_type": "text"
      },
      "source": [
        "### We will read now the images and load them into a Spark Dataframe and them use our transformer to apply the model into the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54547xC3P33U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs = !ls flower_photos/sample/*.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKeNixvFP33Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMJkDh89P33b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uri_df = spark.createDataFrame(fs, StringType()).toDF(\"uri\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sv6441lP33e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uri_df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMARKZGrP33h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dir(transformer)\n",
        "keras_pred_df = transformer.transform(uri_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4w-v5KhP33l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_pred_df.select(\"uri\", \"predictions\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}