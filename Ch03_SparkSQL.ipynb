{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch03_SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRzVuf1Jjvw_"
   },
   "source": [
    "### Open a terminal window and run the following commands:\n",
    "sudo bash\n",
    "start-hadoop\n",
    "cd /home/student/ROI/SparkProgram/Day3\n",
    "./fixhive.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MZuPHFmjvxE"
   },
   "source": [
    "### Let's make a simple hive table for regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gw7WCt1GjvxG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE Regions(\r\n",
      "RegionID int,\r\n",
      "RegionName string)\r\n",
      "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\r\n",
      "\r\n",
      "LOAD DATA LOCAL INPATH '/home/student/ROI/Spark/datasets/northwind/CSV/regions' OVERWRITE INTO TABLE Regions;\r\n",
      "\r\n",
      "select * from regions\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat /home/student/ROI/Spark/regions.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfPR7Z3UjvxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/home/student/ROI/Spark/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISVSWDXVjvxO"
   },
   "source": [
    "### You can query an existing Hive table and bring it into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqn192y6jvxQ"
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nextraneous input ':' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)\\n\\n== SQL ==\\nregionid:int, regionname:string\\n--------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.schema.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input ':' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)\n\n== SQL ==\nregionid:int, regionname:string\n--------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableSchema(ParseDriver.scala:64)\n\tat org.apache.spark.sql.types.StructType$.fromDDL(StructType.scala:449)\n\tat org.apache.spark.sql.DataFrameReader.schema(DataFrameReader.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-109527c88c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(regions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs://localhost:9000/user/hive/warehouse/regions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'regionid:int, regionname:string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'regionid<=2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiLine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             enforceSchema=enforceSchema, emptyValue=emptyValue)\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m_set_opts\u001b[0;34m(self, schema, **options)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mschema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schema should be StructType or string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nextraneous input ':' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)\\n\\n== SQL ==\\nregionid:int, regionname:string\\n--------^^^\\n\""
     ]
    }
   ],
   "source": [
    "regions = spark.sql('select * from regions')\n",
    "# regions.show()\n",
    "# print(regions)\n",
    "\n",
    "# r = spark.read.csv('hdfs://localhost:9000/user/hive/warehouse/regions', schema='regionid:int, regionname:string').where('regionid<=2')\n",
    "# r.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bOc2M8pjvxU"
   },
   "source": [
    "### Read in a file to a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvKK5WJzjvxW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|TerritoryID|TerritoryName|RegionID|\n",
      "+-----------+-------------+--------+\n",
      "|      01581|     Westboro|       1|\n",
      "|      01730|      Bedford|       1|\n",
      "|      01833|    Georgetow|       1|\n",
      "|      02116|       Boston|       1|\n",
      "|      02139|    Cambridge|       1|\n",
      "|      02184|    Braintree|       1|\n",
      "|      02903|   Providence|       1|\n",
      "|      03049|       Hollis|       3|\n",
      "|      03801|   Portsmouth|       3|\n",
      "|      06897|       Wilton|       1|\n",
      "|      07960|   Morristown|       1|\n",
      "|      08837|       Edison|       1|\n",
      "|      10019|     New York|       1|\n",
      "|      10038|     New York|       1|\n",
      "|      11747|     Mellvile|       1|\n",
      "|      14450|     Fairport|       1|\n",
      "|      19428| Philadelphia|       3|\n",
      "|      19713|       Neward|       1|\n",
      "|      20852|    Rockville|       1|\n",
      "|      27403|   Greensboro|       1|\n",
      "+-----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "territories = spark.read.csv(f'{datapath}/northwind/CSVHeaders/territories', header=True)\n",
    "territories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YejFbgM_jvxZ"
   },
   "source": [
    "### Use createOrReplaceTempView to create a virtual table in the Hive catalog and then it can be queried using SQL as if it were a hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DY_yVNWjvxb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|TerritoryID|TerritoryName|RegionID|\n",
      "+-----------+-------------+--------+\n",
      "|      01730|      Bedford|       1|\n",
      "|      02116|       Boston|       1|\n",
      "|      02184|    Braintree|       1|\n",
      "|      02139|    Cambridge|       1|\n",
      "|      27511|         Cary|       1|\n",
      "|      08837|       Edison|       1|\n",
      "|      14450|     Fairport|       1|\n",
      "|      01833|    Georgetow|       1|\n",
      "|      27403|   Greensboro|       1|\n",
      "|      40222|   Louisville|       1|\n",
      "|      11747|     Mellvile|       1|\n",
      "|      07960|   Morristown|       1|\n",
      "|      10019|     New York|       1|\n",
      "|      10038|     New York|       1|\n",
      "|      19713|       Neward|       1|\n",
      "|      02903|   Providence|       1|\n",
      "|      20852|    Rockville|       1|\n",
      "|      01581|     Westboro|       1|\n",
      "|      06897|       Wilton|       1|\n",
      "+-----------+-------------+--------+\n",
      "\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "territories.createOrReplaceTempView('territories')\n",
    "t1 =spark.sql('select * from territories where regionid = 1').orderBy('TerritoryName')\n",
    "t1.show()\n",
    "print(t1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cuO52Zzjvxf"
   },
   "source": [
    "### Spark DataFrames can be saved to a Hive table using either the saveAsTable method or writing a SQL query that uses CREATE TABLE AS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mU5P_oxAjvxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/hive/warehouse/territories2': No such file or directory\n",
      "rm: `/user/hive/warehouse/territories3': No such file or directory\n",
      "rm: `/user/hive/warehouse/territoryregion': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/hive/warehouse/territories2\n",
    "! hadoop fs -rm -r /user/hive/warehouse/territories3\n",
    "! hadoop fs -rm -r /user/hive/warehouse/territoryregion\n",
    "spark.sql('drop table if exists territories2')\n",
    "spark.sql('drop table if exists territories3')\n",
    "\n",
    "territories.write.saveAsTable('Territories2', mode='overwrite')\n",
    "spark.sql('create table Territories3 as select * from territories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mh-n9ExPjvxl"
   },
   "source": [
    "### Queries use standard HQL to mix Hive tables and virtual tables. Both are read into a Spark DataFrame and the processing happens at the Spark level not at the Hive level. HQL is just used to parse the logic into the corresponding Spark methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhTD011pjvxm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+-------------+\n",
      "|regionid|regionname|territoryid|territoryname|\n",
      "+--------+----------+-----------+-------------+\n",
      "|       1|   Eastern|      01581|     Westboro|\n",
      "|       1|   Eastern|      01730|      Bedford|\n",
      "|       1|   Eastern|      01833|    Georgetow|\n",
      "|       1|   Eastern|      02116|       Boston|\n",
      "|       1|   Eastern|      02139|    Cambridge|\n",
      "|       1|   Eastern|      02184|    Braintree|\n",
      "|       1|   Eastern|      02903|   Providence|\n",
      "|       1|   Eastern|      06897|       Wilton|\n",
      "|       1|   Eastern|      07960|   Morristown|\n",
      "|       1|   Eastern|      08837|       Edison|\n",
      "+--------+----------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+----------+-----------+-------------+\n",
      "|regionid|regionname|TerritoryID|TerritoryName|\n",
      "+--------+----------+-----------+-------------+\n",
      "|       1|   Eastern|      01581|     Westboro|\n",
      "|       1|   Eastern|      01730|      Bedford|\n",
      "|       1|   Eastern|      01833|    Georgetow|\n",
      "|       1|   Eastern|      02116|       Boston|\n",
      "|       1|   Eastern|      02139|    Cambridge|\n",
      "|       1|   Eastern|      02184|    Braintree|\n",
      "|       1|   Eastern|      02903|   Providence|\n",
      "|       1|   Eastern|      06897|       Wilton|\n",
      "|       1|   Eastern|      07960|   Morristown|\n",
      "|       1|   Eastern|      08837|       Edison|\n",
      "+--------+----------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select r.regionid, r.regionname, t.territoryid, t.territoryname \n",
    "from regions as r \n",
    "join territories as t on r.regionid = t.regionid \n",
    "order by r.regionid, t.territoryid\n",
    "\"\"\"\n",
    "rt = spark.sql(sql)\n",
    "rt.show(10)\n",
    "\n",
    "tr = regions.join(territories, regions.regionid == territories.RegionID). \\\n",
    "     select('regions.regionid', 'regionname', 'TerritoryID', 'TerritoryName')\n",
    "tr.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIhleFVQjvxq"
   },
   "source": [
    "### Lab: Read the northwind JSON products and make it into a TempView and do the same with the CSVHeaders version of categories. Then join the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1W3KqQamjvxr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[CategoryID: int, CategoryName: string, Description: string]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Soft drinks coffees teas beers and ales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Condiments</td>\n",
       "      <td>Sweet and savory sauces relishes spreads and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Desserts candies and sweet breads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dairy Products</td>\n",
       "      <td>Cheeses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Grains/Cereals</td>\n",
       "      <td>Breads crackers pasta and cereal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Meat/Poultry</td>\n",
       "      <td>Prepared meats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Produce</td>\n",
       "      <td>Dried fruit and bean curd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Seafood</td>\n",
       "      <td>Seaweed and fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID    CategoryName  \\\n",
       "0           1       Beverages   \n",
       "1           2      Condiments   \n",
       "2           3     Confections   \n",
       "3           4  Dairy Products   \n",
       "4           5  Grains/Cereals   \n",
       "5           6    Meat/Poultry   \n",
       "6           7         Produce   \n",
       "7           8         Seafood   \n",
       "\n",
       "                                         Description  \n",
       "0            Soft drinks coffees teas beers and ales  \n",
       "1  Sweet and savory sauces relishes spreads and s...  \n",
       "2                  Desserts candies and sweet breads  \n",
       "3                                            Cheeses  \n",
       "4                   Breads crackers pasta and cereal  \n",
       "5                                     Prepared meats  \n",
       "6                          Dried fruit and bean curd  \n",
       "7                                   Seaweed and fish  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[categoryid: bigint, discontinued: bigint, productid: bigint, productname: string, quantityperunit: string, reorderlevel: bigint, supplierid: bigint, unitprice: double, unitsinstock: bigint, unitsonorder: bigint]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryid</th>\n",
       "      <th>discontinued</th>\n",
       "      <th>productid</th>\n",
       "      <th>productname</th>\n",
       "      <th>quantityperunit</th>\n",
       "      <th>reorderlevel</th>\n",
       "      <th>supplierid</th>\n",
       "      <th>unitprice</th>\n",
       "      <th>unitsinstock</th>\n",
       "      <th>unitsonorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Chai</td>\n",
       "      <td>10 boxes x 30 bags</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>18.00</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Chang</td>\n",
       "      <td>24 - 12 oz bottles</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Aniseed Syrup</td>\n",
       "      <td>12 - 550 ml bottles</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Chef Anton's Cajun Seasoning</td>\n",
       "      <td>48 - 6 oz jars</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.00</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Chef Anton's Gumbo Mix</td>\n",
       "      <td>36 boxes</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Grandma's Boysenberry Spread</td>\n",
       "      <td>12 - 8 oz jars</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>25.00</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Uncle Bob's Organic Dried Pears</td>\n",
       "      <td>12 - 1 lb pkgs.</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>30.00</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Northwoods Cranberry Sauce</td>\n",
       "      <td>12 - 12 oz jars</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Mishi Kobe Niku</td>\n",
       "      <td>18 - 500 g pkgs.</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.00</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Ikura</td>\n",
       "      <td>12 - 200 ml jars</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>31.00</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categoryid  discontinued  productid                      productname  \\\n",
       "0           1             1          1                             Chai   \n",
       "1           1             1          2                            Chang   \n",
       "2           2             0          3                    Aniseed Syrup   \n",
       "3           2             0          4     Chef Anton's Cajun Seasoning   \n",
       "4           2             1          5           Chef Anton's Gumbo Mix   \n",
       "5           2             0          6     Grandma's Boysenberry Spread   \n",
       "6           7             0          7  Uncle Bob's Organic Dried Pears   \n",
       "7           2             0          8       Northwoods Cranberry Sauce   \n",
       "8           6             1          9                  Mishi Kobe Niku   \n",
       "9           8             0         10                            Ikura   \n",
       "\n",
       "       quantityperunit  reorderlevel  supplierid  unitprice  unitsinstock  \\\n",
       "0   10 boxes x 30 bags            10           8      18.00            39   \n",
       "1   24 - 12 oz bottles            25           1      19.00            17   \n",
       "2  12 - 550 ml bottles            25           1      10.00            13   \n",
       "3       48 - 6 oz jars             0           2      22.00            53   \n",
       "4             36 boxes             0           2      21.35             0   \n",
       "5       12 - 8 oz jars            25           3      25.00           120   \n",
       "6      12 - 1 lb pkgs.            10           3      30.00            15   \n",
       "7      12 - 12 oz jars             0           3      40.00             6   \n",
       "8     18 - 500 g pkgs.             0           4      97.00            29   \n",
       "9     12 - 200 ml jars             0           4      31.00            31   \n",
       "\n",
       "   unitsonorder  \n",
       "0             0  \n",
       "1            40  \n",
       "2            70  \n",
       "3             0  \n",
       "4             0  \n",
       "5             0  \n",
       "6             0  \n",
       "7             0  \n",
       "8             0  \n",
       "9             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryid</th>\n",
       "      <th>categoryname</th>\n",
       "      <th>productid</th>\n",
       "      <th>productname</th>\n",
       "      <th>unitprice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>1</td>\n",
       "      <td>Chai</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>2</td>\n",
       "      <td>Chang</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>24</td>\n",
       "      <td>Guarana Fantastica</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>34</td>\n",
       "      <td>Sasquatch Ale</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>35</td>\n",
       "      <td>Steeleye Stout</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>38</td>\n",
       "      <td>Cote de Blaye</td>\n",
       "      <td>263.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>39</td>\n",
       "      <td>Chartreuse verte</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>43</td>\n",
       "      <td>Ipoh Coffee</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>67</td>\n",
       "      <td>Laughing Lumberjack Lager</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>70</td>\n",
       "      <td>Outback Lager</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categoryid categoryname  productid                productname  unitprice\n",
       "0           1    Beverages          1                       Chai       18.0\n",
       "1           1    Beverages          2                      Chang       19.0\n",
       "2           1    Beverages         24         Guarana Fantastica        4.5\n",
       "3           1    Beverages         34              Sasquatch Ale       14.0\n",
       "4           1    Beverages         35             Steeleye Stout       18.0\n",
       "5           1    Beverages         38              Cote de Blaye      263.5\n",
       "6           1    Beverages         39           Chartreuse verte       18.0\n",
       "7           1    Beverages         43                Ipoh Coffee       46.0\n",
       "8           1    Beverages         67  Laughing Lumberjack Lager       14.0\n",
       "9           1    Beverages         70              Outback Lager       15.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = spark.read.csv(f'{datapath}/northwind/CSVHeaders/categories', header=True, inferSchema = True)\n",
    "print(categories)\n",
    "display(categories)\n",
    "categories.createOrReplaceTempView('categories') \n",
    "\n",
    "products = spark.read.json(f'{datapath}/northwind/JSON/products')\n",
    "print(products)\n",
    "display(products)\n",
    "products.createOrReplaceTempView('products') \n",
    "\n",
    "sql = '''\n",
    "select c.categoryid, c.categoryname, p.productid, p.productname, p.unitprice\n",
    "from products as p\n",
    "join categories as c on p.categoryid = c.categoryid\n",
    "order by c.categoryid, p.productid\n",
    "'''\n",
    "display(spark.sql(sql))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0fxc66Zjvxv"
   },
   "source": [
    "### Install the MySQL Python connector. This has nothing to do with Spark but if you want to run SQL queries directly, it is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cafLHRpJjvxw"
   },
   "outputs": [],
   "source": [
    "#! pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYdB-iOyjvx0"
   },
   "source": [
    "### Let's make sure we have a database for northwind and no regions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pmtrb-VAjvx2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "try:\n",
    "    cn = mysql.connector.connect(host='localhost', user='test', password='password')\n",
    "    cursor = cn.cursor()\n",
    "    cursor.execute('create database if not exists northwind')\n",
    "    cn.close()\n",
    "\n",
    "    cn = mysql.connector.connect(host='localhost', user='test', password='password', database='northwind')\n",
    "    cursor = cn.cursor()    \n",
    "    cursor.execute('drop table if exists regions')\n",
    "    cn.close()\n",
    "except:\n",
    "    print('something went wrong')\n",
    "else:\n",
    "    print('success')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N36mOVI-jvx6"
   },
   "source": [
    "### Write a DataFrame to a SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u666dY6gjvx7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "regions.write.format(\"jdbc\").mode('overwrite').options(url=\"jdbc:mysql://localhost/northwind\", driver='com.mysql.jdbc.Driver', dbtable='regions', user='test', password = \"password\", mode = \"append\", useSSL = \"false\").save()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTyFO5Exjvx_"
   },
   "source": [
    "### Read a SQL table into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVi4NdT0jvyA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|regionid|   regionname|\n",
      "+--------+-------------+\n",
      "|       1|      Eastern|\n",
      "|       2|      Western|\n",
      "|       3|     Northern|\n",
      "|       4|     Southern|\n",
      "|       5|South America|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions2 = spark.read.format(\"jdbc\").options(url=\"jdbc:mysql://localhost/northwind\", driver=\"com.mysql.jdbc.Driver\", dbtable= \"regions\", user=\"test\", password=\"password\").load()\n",
    "regions2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNfDRz36jvyF"
   },
   "source": [
    "### Creating the regions2 DataFrame does not execute anything yet, but by making the DataFrame into a Temp View then running a Spark SQL query, it tells Spark to read the SQL data into a DataFrame and then use the cluster to do the processing, not the SQL source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROA9gHJ9jvyG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|regionid|regionname|\n",
      "+--------+----------+\n",
      "|       1|   Eastern|\n",
      "|       2|   Western|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions2.createOrReplaceTempView('regions2')\n",
    "spark.sql('select * from regions2 where regionid < 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uY5szGj1jvyJ"
   },
   "source": [
    "### Alternate ways to code a query using SQL and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TumW8dZpjvyK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryid</th>\n",
       "      <th>discontinued</th>\n",
       "      <th>productid</th>\n",
       "      <th>productname</th>\n",
       "      <th>quantityperunit</th>\n",
       "      <th>reorderlevel</th>\n",
       "      <th>supplierid</th>\n",
       "      <th>unitprice</th>\n",
       "      <th>unitsinstock</th>\n",
       "      <th>unitsonorder</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Chai</td>\n",
       "      <td>10 boxes x 30 bags</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>18.00</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>702.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Chef Anton's Cajun Seasoning</td>\n",
       "      <td>48 - 6 oz jars</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.00</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1166.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Grandma's Boysenberry Spread</td>\n",
       "      <td>12 - 8 oz jars</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>25.00</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>3000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Mishi Kobe Niku</td>\n",
       "      <td>18 - 500 g pkgs.</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.00</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2813.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Ikura</td>\n",
       "      <td>12 - 200 ml jars</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>31.00</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>961.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>Queso Manchego La Pastora</td>\n",
       "      <td>10 - 500 g pkgs.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>38.00</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>3268.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>Tofu</td>\n",
       "      <td>40 - 100 g pkgs.</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23.25</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>813.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>Genen Shouyu</td>\n",
       "      <td>24 - 250 ml bottles</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>13.00</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>507.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Pavlova</td>\n",
       "      <td>32 - 500 g boxes</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>17.45</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>506.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>Carnarvon Tigers</td>\n",
       "      <td>16 kg pkg.</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>62.50</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2625.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categoryid  discontinued  productid                   productname  \\\n",
       "0           1             1          1                          Chai   \n",
       "1           2             0          4  Chef Anton's Cajun Seasoning   \n",
       "2           2             0          6  Grandma's Boysenberry Spread   \n",
       "3           6             1          9               Mishi Kobe Niku   \n",
       "4           8             0         10                         Ikura   \n",
       "5           4             0         12     Queso Manchego La Pastora   \n",
       "6           7             0         14                          Tofu   \n",
       "7           2             0         15                  Genen Shouyu   \n",
       "8           3             0         16                       Pavlova   \n",
       "9           8             0         18              Carnarvon Tigers   \n",
       "\n",
       "       quantityperunit  reorderlevel  supplierid  unitprice  unitsinstock  \\\n",
       "0   10 boxes x 30 bags            10           8      18.00            39   \n",
       "1       48 - 6 oz jars             0           2      22.00            53   \n",
       "2       12 - 8 oz jars            25           3      25.00           120   \n",
       "3     18 - 500 g pkgs.             0           4      97.00            29   \n",
       "4     12 - 200 ml jars             0           4      31.00            31   \n",
       "5     10 - 500 g pkgs.             0           5      38.00            86   \n",
       "6     40 - 100 g pkgs.             0           6      23.25            35   \n",
       "7  24 - 250 ml bottles             5           6      13.00            39   \n",
       "8     32 - 500 g boxes            10           7      17.45            29   \n",
       "9           16 kg pkg.             0           7      62.50            42   \n",
       "\n",
       "   unitsonorder    value  \n",
       "0             0   702.00  \n",
       "1             0  1166.00  \n",
       "2             0  3000.00  \n",
       "3             0  2813.00  \n",
       "4             0   961.00  \n",
       "5             0  3268.00  \n",
       "6             0   813.75  \n",
       "7             0   507.00  \n",
       "8             0   506.05  \n",
       "9             0  2625.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x = spark.sql('select count(*) from regions').collect()\n",
    "# print(x[0][0])\n",
    "# spark.sql('select * from regions').count()\n",
    "\n",
    "p2 = products.withColumn('value', products.unitprice * products.unitsinstock).where('value > 500')\n",
    "display(p2)\n",
    "\n",
    "sql = \"\"\"\n",
    "select *\n",
    "from (select *, unitprice * quantity as value) as t\n",
    "where value > 500\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNL1sa6ljvyN"
   },
   "source": [
    "### Using SQL you can use familiar syntax instead of withColumn or withCoumnRenamed methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8HXp3WzjvyP"
   },
   "outputs": [],
   "source": [
    "t1 = spark.sql('select TerritoryID as TerrID, UPPER(TerritoryName) as TerritoryName, RegionID from territories')\n",
    "t1.show(5)\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "territories.withColumn('TerritoryName', expr('UPPER(TerritoryName)')).withColumnRenamed('TerritoryID', 'TerrID').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXOWeUhCjvyT"
   },
   "source": [
    "### Sometimes there is a function in Python that doesn't exist in SQL and it would be helpful to use, so you could make a udf and use withColumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7aDNl6GjvyU"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "t2 = spark.sql('select * from territories')\n",
    "t2.printSchema()\n",
    "#t2.show()\n",
    "t2 = t2.withColumn('upperName', expr('UPPER(TerritoryName)'))\n",
    "t2.show(5)\n",
    "\n",
    "t2 = t2.withColumn('titleName', udf(lambda x : x.title(), StringType())(t2.upperName))\n",
    "t2.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Hyt_WXFjvyh"
   },
   "source": [
    "### To make it easier though, you could make the Python function into a udf that SQL can understand similar to how you can make a DataFrame seem like a virtual table with createOrReplaceTempView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKyjaetvjvyl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+------------+\n",
      "|TerritoryID|TerritoryName|RegionID|    Reversed|\n",
      "+-----------+-------------+--------+------------+\n",
      "|      90405| Santa Monica|       2|acinoM atnaS|\n",
      "|      29202|     Columbia|       4|    aibmuloC|\n",
      "|      19428| Philadelphia|       3|aihpledalihP|\n",
      "|      33607|        Tampa|       4|       apmaT|\n",
      "|      95054|  Santa Clara|       2| aralC atnaS|\n",
      "|      30346|      Atlanta|       4|     atnaltA|\n",
      "|      48075|   Southfield|       3|  dleifhtuoS|\n",
      "|      98052|      Redmond|       2|     dnomdeR|\n",
      "|      44122|    Beachwood|       3|   doowhcaeB|\n",
      "|      19713|       Neward|       1|      draweN|\n",
      "|      01730|      Bedford|       1|     drofdeB|\n",
      "|      02903|   Providence|       1|  ecnedivorP|\n",
      "|      02184|    Braintree|       1|   eertniarB|\n",
      "|      02139|    Cambridge|       1|   egdirbmaC|\n",
      "|      85251|   Scottsdale|       2|  eladsttocS|\n",
      "|      11747|     Mellvile|       1|    elivlleM|\n",
      "|      55113|    Roseville|       3|   ellivesoR|\n",
      "|      20852|    Rockville|       1|   ellivkcoR|\n",
      "|      72716|  Bentonville|       4| ellivnotneB|\n",
      "|      40222|   Louisville|       1|  ellivsiuoL|\n",
      "+-----------+-------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reverseString(x):\n",
    "    return x[::-1]\n",
    "\n",
    "spark.udf.register('reverse', reverseString, StringType())\n",
    "\n",
    "spark.sql('select *, reverse(TerritoryName) as Reversed from Territories').orderBy('Reversed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvgexA38jvyq"
   },
   "source": [
    "### HQL has collect_set and collect_list functions to aggregate items into a list instead of summing them up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO1-BX2Yjvyt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------+\n",
      "|RegionID|collect_list(TerritoryName)|\n",
      "+--------+---------------------------+\n",
      "|       3|       [Hollis, Portsmou...|\n",
      "|       1|       [Westboro, Bedfor...|\n",
      "|       4|       [Columbia, Atlant...|\n",
      "|       2|       [Hoffman Estates,...|\n",
      "+--------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "territories.groupBy(territories.RegionID).agg(collect_list(territories.TerritoryName)).show()\n",
    "\n",
    "tr1 = spark.sql(\"SELECT RegionID, collect_list(TerritoryName) AS TerritoryList FROM Territories GROUP BY RegionID\")\n",
    "display(tr1)\n",
    "tr1.printSchema()\n",
    "print(tr1.take(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bER0vib7jvyw"
   },
   "source": [
    "### Instead of a simple datatype you could also collect complex structured objects using the HQL NAMED_STRUCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBw7Xyphjvyx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RegionID: integer (nullable = true)\n",
      " |-- RegionName: string (nullable = true)\n",
      " |-- TerritoryList: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- TerritoryID: string (nullable = true)\n",
      " |    |    |-- TerritoryName: string (nullable = true)\n",
      "\n",
      "DataFrame[RegionID: int, RegionName: string, TerritoryList: array<struct<TerritoryID:string,TerritoryName:string>>]\n",
      "+--------+----------+--------------------+\n",
      "|RegionID|RegionName|       TerritoryList|\n",
      "+--------+----------+--------------------+\n",
      "|       1|   Eastern|[[27511, Cary], [...|\n",
      "|       2|   Western|[[60179, Hoffman ...|\n",
      "|       3|  Northern|[[45839, Findlay]...|\n",
      "|       4|  Southern|[[75234, Dallas],...|\n",
      "+--------+----------+--------------------+\n",
      "\n",
      "[Row(RegionID=1, RegionName='Eastern', TerritoryList=[Row(TerritoryID='27511', TerritoryName='Cary'), Row(TerritoryID='07960', TerritoryName='Morristown'), Row(TerritoryID='02184', TerritoryName='Braintree'), Row(TerritoryID='11747', TerritoryName='Mellvile'), Row(TerritoryID='01581', TerritoryName='Westboro'), Row(TerritoryID='14450', TerritoryName='Fairport'), Row(TerritoryID='20852', TerritoryName='Rockville'), Row(TerritoryID='10038', TerritoryName='New York'), Row(TerritoryID='02903', TerritoryName='Providence'), Row(TerritoryID='02116', TerritoryName='Boston'), Row(TerritoryID='06897', TerritoryName='Wilton'), Row(TerritoryID='19713', TerritoryName='Neward'), Row(TerritoryID='40222', TerritoryName='Louisville'), Row(TerritoryID='27403', TerritoryName='Greensboro'), Row(TerritoryID='08837', TerritoryName='Edison'), Row(TerritoryID='01833', TerritoryName='Georgetow'), Row(TerritoryID='01730', TerritoryName='Bedford'), Row(TerritoryID='10019', TerritoryName='New York'), Row(TerritoryID='02139', TerritoryName='Cambridge')]), Row(RegionID=2, RegionName='Western', TerritoryList=[Row(TerritoryID='60179', TerritoryName='Hoffman Estates'), Row(TerritoryID='94025', TerritoryName='Menlo Park'), Row(TerritoryID='95054', TerritoryName='Santa Clara'), Row(TerritoryID='98104', TerritoryName='Seattle'), Row(TerritoryID='90405', TerritoryName='Santa Monica'), Row(TerritoryID='98004', TerritoryName='Bellevue'), Row(TerritoryID='85014', TerritoryName='Phoenix'), Row(TerritoryID='95060', TerritoryName='Santa Cruz'), Row(TerritoryID='98052', TerritoryName='Redmond'), Row(TerritoryID='60601', TerritoryName='Chicago'), Row(TerritoryID='94105', TerritoryName='San Francisco'), Row(TerritoryID='80909', TerritoryName='Colorado Springs'), Row(TerritoryID='95008', TerritoryName='Campbell'), Row(TerritoryID='80202', TerritoryName='Denver'), Row(TerritoryID='85251', TerritoryName='Scottsdale')])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sql = \"\"\"SELECT r.RegionID, r.RegionName\n",
    ", COLLECT_SET(NAMED_STRUCT(\"TerritoryID\", t.TerritoryID, \"TerritoryName\", t.TerritoryName)) AS TerritoryList\n",
    "FROM Regions AS r\n",
    "JOIN Territories AS t ON r.RegionID = t.RegionID\n",
    "GROUP BY r.RegionID, r.RegionName\n",
    "ORDER BY r.RegionID\"\"\"\n",
    "\n",
    "tr2 = spark.sql(sql)\n",
    "tr2.printSchema()\n",
    "print(tr2)\n",
    "tr2.show()\n",
    "print(tr2.take(2))\n",
    "tr2.write.json('TerritoryRegion.json')\n",
    "spark.sql('create table TerritoryRegion as ' + sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>TerritoryList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>[(27511, Cary), (07960, Morristown), (02184, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Western</td>\n",
       "      <td>[(60179, Hoffman Estates), (94025, Menlo Park)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Northern</td>\n",
       "      <td>[(45839, Findlay), (48304, Bloomfield Hills), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Southern</td>\n",
       "      <td>[(75234, Dallas), (32859, Orlando), (30346, At...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RegionID RegionName                                      TerritoryList\n",
       "0         1    Eastern  [(27511, Cary), (07960, Morristown), (02184, B...\n",
       "1         2    Western  [(60179, Hoffman Estates), (94025, Menlo Park)...\n",
       "2         3   Northern  [(45839, Findlay), (48304, Bloomfield Hills), ...\n",
       "3         4   Southern  [(75234, Dallas), (32859, Orlando), (30346, At..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RegionID: long (nullable = true)\n",
      " |-- RegionName: string (nullable = true)\n",
      " |-- TerritoryList: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- TerritoryID: string (nullable = true)\n",
      " |    |    |-- TerritoryName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr2 = spark.read.json('TerritoryRegion.json')\n",
    "display(tr2)\n",
    "tr2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFruFeUfjvy5"
   },
   "source": [
    "### If you have data that is already collected into a complex datatype and want to flatten it, you could use HQL EXPLODE function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrFANoYojvy6"
   },
   "source": [
    "### You could use the Spark explode method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFsijdUfjvy7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|RegionID|   TerritoryName|\n",
      "+--------+----------------+\n",
      "|       3|          Hollis|\n",
      "|       3|      Portsmouth|\n",
      "|       3|    Philadelphia|\n",
      "|       3|       Beachwood|\n",
      "|       3|         Findlay|\n",
      "|       3|      Southfield|\n",
      "|       3|            Troy|\n",
      "|       3|Bloomfield Hills|\n",
      "|       3|          Racine|\n",
      "|       3|       Roseville|\n",
      "|       3|     Minneapolis|\n",
      "|       1|        Westboro|\n",
      "|       1|         Bedford|\n",
      "|       1|       Georgetow|\n",
      "|       1|          Boston|\n",
      "|       1|       Cambridge|\n",
      "|       1|       Braintree|\n",
      "|       1|      Providence|\n",
      "|       1|          Wilton|\n",
      "|       1|      Morristown|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "tr1.select('RegionID', explode('TerritoryList')).withColumnRenamed('col','TerritoryName').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBKRyK65jvy-"
   },
   "source": [
    "### Or if the DataFrame is turned into a Temp View, you could use the HQL query to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u96Thbgjvy_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|RegionID|TerritoryName|\n",
      "+--------+-------------+\n",
      "|       1|      Bedford|\n",
      "|       1|       Boston|\n",
      "|       1|    Braintree|\n",
      "|       1|    Cambridge|\n",
      "|       1|         Cary|\n",
      "|       1|       Edison|\n",
      "|       1|     Fairport|\n",
      "|       1|    Georgetow|\n",
      "|       1|   Greensboro|\n",
      "|       1|   Louisville|\n",
      "|       1|     Mellvile|\n",
      "|       1|   Morristown|\n",
      "|       1|     New York|\n",
      "|       1|     New York|\n",
      "|       1|       Neward|\n",
      "|       1|   Providence|\n",
      "|       1|    Rockville|\n",
      "|       1|     Westboro|\n",
      "|       1|       Wilton|\n",
      "|       2|     Bellevue|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr1.createOrReplaceTempView('RegionTerritories')\n",
    "sql = \"\"\"SELECT RegionID, TerritoryName\n",
    "FROM RegionTerritories\n",
    "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS TerritoryName\n",
    "ORDER BY RegionID, TerritoryName\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Grkd6ghFjvzC"
   },
   "source": [
    "### Or you could select specific elements from a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwtqHRLijvzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+-------------------+--------------+\n",
      "|RegionId|RegionName|               First|               Last|TerritoryCount|\n",
      "+--------+----------+--------------------+-------------------+--------------+\n",
      "|       1|   Eastern|       [27511, Cary]| [02139, Cambridge]|            19|\n",
      "|       2|   Western|[60179, Hoffman E...|[85251, Scottsdale]|            15|\n",
      "|       3|  Northern|    [45839, Findlay]| [44122, Beachwood]|            11|\n",
      "|       4|  Southern|     [75234, Dallas]|     [33607, Tampa]|             8|\n",
      "+--------+----------+--------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr2.createOrReplaceTempView('RegionTerritories')\n",
    "spark.sql(\"select RegionId, RegionName, TerritoryList[0] as First, TerritoryList[size(TerritoryList) - 1] as Last, size(TerritoryList) as TerritoryCount from RegionTerritories\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXZWzUozjvzG"
   },
   "source": [
    "### If the array is of structs note the syntax of fetching the elements from the struct uses the . like an object property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jf9WqBvnjvzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|RegionID|RegionName|TerritoryID|  TerritoryName|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|   Eastern|      27511|           Cary|\n",
      "|       1|   Eastern|      07960|     Morristown|\n",
      "|       1|   Eastern|      02184|      Braintree|\n",
      "|       1|   Eastern|      11747|       Mellvile|\n",
      "|       1|   Eastern|      01581|       Westboro|\n",
      "|       1|   Eastern|      14450|       Fairport|\n",
      "|       1|   Eastern|      20852|      Rockville|\n",
      "|       1|   Eastern|      10038|       New York|\n",
      "|       1|   Eastern|      02903|     Providence|\n",
      "|       1|   Eastern|      02116|         Boston|\n",
      "|       1|   Eastern|      06897|         Wilton|\n",
      "|       1|   Eastern|      19713|         Neward|\n",
      "|       1|   Eastern|      40222|     Louisville|\n",
      "|       1|   Eastern|      27403|     Greensboro|\n",
      "|       1|   Eastern|      08837|         Edison|\n",
      "|       1|   Eastern|      01833|      Georgetow|\n",
      "|       1|   Eastern|      01730|        Bedford|\n",
      "|       1|   Eastern|      10019|       New York|\n",
      "|       1|   Eastern|      02139|      Cambridge|\n",
      "|       2|   Western|      60179|Hoffman Estates|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"SELECT RegionID, RegionName, Territory.TerritoryID AS TerritoryID, Territory.TerritoryName AS TerritoryName\n",
    "FROM RegionTerritories\n",
    "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS Territory\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mi0Wyy8rjvzM"
   },
   "source": [
    "### Homework ##\n",
    "\n",
    "**First Challenge**\n",
    "\n",
    "Create a Python function to determine if a number is odd or even and use that to select only the even numbered shippers from the TSV folder of northwind. Note the TSV file does not have headers so you will need to do something to make the DataFrame have a meaningful structure. I would suggest using SparkSql as much as possible to rename and cast the columns which are ShipperID, CompanyName and Phone.\n",
    "\n",
    "**Second Challenge**\n",
    "\n",
    "Take the Order_LineItems.json folder, read it into a DataFrame, and flatten it and then calculate the average price paid for a product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBcR1OCOjvzN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Read the following code and see how it will shape order line items into the order header record\n",
    "# You will use the result of this saved file for the second challenge\n",
    "o = spark.read.csv('/home/student/ROI/Spark/datasets/northwind/CSVHeaders/orders', header = True, inferSchema = True)\n",
    "od = spark.read.csv('/home/student/ROI/Spark/datasets/northwind/CSVHeaders/orderdetails', header = True, inferSchema = True)\n",
    "\n",
    "o.createOrReplaceTempView('Orders')\n",
    "od.createOrReplaceTempView('OrderDetails')\n",
    "sql = \"\"\"\n",
    "select o.OrderID, o.CustomerID, o.OrderDate\n",
    "           , COUNT(*) as cnt\n",
    "           , SUM(od.unitprice * od.quantity) as OrderTotal\n",
    "           , COLLECT_SET(NAMED_STRUCT(\"ProductID\", od.ProductID, \n",
    "                                      \"UnitPrice\", od.UnitPrice,\n",
    "                                      \"Quantity\", od.Quantity,\n",
    "                                      \"Discount\", od.discount)) as LineItems\n",
    "from Orders as o join OrderDetails as od on o.OrderID = od.OrderID\n",
    "GROUP BY o.OrderID, o.CustomerID, o.OrderDate\n",
    "ORDER BY o.OrderID\"\"\"\n",
    "od2 = spark.sql(sql)\n",
    "od2.write.mode('overwrite').json('Orders_LineItems.json')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>cnt</th>\n",
       "      <th>OrderTotal</th>\n",
       "      <th>LineItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10248</td>\n",
       "      <td>VINET</td>\n",
       "      <td>1996-07-04</td>\n",
       "      <td>3</td>\n",
       "      <td>440.0</td>\n",
       "      <td>[(72, 34.8, 5, 0.0), (42, 9.8, 10, 0.0), (11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10249</td>\n",
       "      <td>TOMSP</td>\n",
       "      <td>1996-07-05</td>\n",
       "      <td>2</td>\n",
       "      <td>1863.4</td>\n",
       "      <td>[(51, 42.4, 40, 0.0), (14, 18.6, 9, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10250</td>\n",
       "      <td>HANAR</td>\n",
       "      <td>1996-07-08</td>\n",
       "      <td>3</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>[(65, 16.8, 15, 0.15), (51, 42.4, 35, 0.15), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10251</td>\n",
       "      <td>VICTE</td>\n",
       "      <td>1996-07-08</td>\n",
       "      <td>3</td>\n",
       "      <td>670.8</td>\n",
       "      <td>[(22, 16.8, 6, 0.05), (57, 15.6, 15, 0.05), (6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10252</td>\n",
       "      <td>SUPRD</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>3</td>\n",
       "      <td>3730.0</td>\n",
       "      <td>[(33, 2.0, 25, 0.05), (20, 64.8, 40, 0.05), (6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10253</td>\n",
       "      <td>HANAR</td>\n",
       "      <td>1996-07-10</td>\n",
       "      <td>3</td>\n",
       "      <td>1444.8</td>\n",
       "      <td>[(31, 10.0, 20, 0.0), (49, 16.0, 40, 0.0), (39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10254</td>\n",
       "      <td>CHOPS</td>\n",
       "      <td>1996-07-11</td>\n",
       "      <td>3</td>\n",
       "      <td>625.2</td>\n",
       "      <td>[(55, 19.2, 21, 0.15), (24, 3.6, 15, 0.15), (7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10255</td>\n",
       "      <td>RICSU</td>\n",
       "      <td>1996-07-12</td>\n",
       "      <td>4</td>\n",
       "      <td>2490.5</td>\n",
       "      <td>[(16, 13.9, 35, 0.0), (59, 44.0, 30, 0.0), (2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10256</td>\n",
       "      <td>WELLI</td>\n",
       "      <td>1996-07-15</td>\n",
       "      <td>2</td>\n",
       "      <td>517.8</td>\n",
       "      <td>[(77, 10.4, 12, 0.0), (53, 26.2, 15, 0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10257</td>\n",
       "      <td>HILAA</td>\n",
       "      <td>1996-07-16</td>\n",
       "      <td>3</td>\n",
       "      <td>1119.9</td>\n",
       "      <td>[(27, 35.1, 25, 0.0), (77, 10.4, 15, 0.0), (39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrderID CustomerID  OrderDate  cnt  OrderTotal  \\\n",
       "0    10248      VINET 1996-07-04    3       440.0   \n",
       "1    10249      TOMSP 1996-07-05    2      1863.4   \n",
       "2    10250      HANAR 1996-07-08    3      1813.0   \n",
       "3    10251      VICTE 1996-07-08    3       670.8   \n",
       "4    10252      SUPRD 1996-07-09    3      3730.0   \n",
       "5    10253      HANAR 1996-07-10    3      1444.8   \n",
       "6    10254      CHOPS 1996-07-11    3       625.2   \n",
       "7    10255      RICSU 1996-07-12    4      2490.5   \n",
       "8    10256      WELLI 1996-07-15    2       517.8   \n",
       "9    10257      HILAA 1996-07-16    3      1119.9   \n",
       "\n",
       "                                           LineItems  \n",
       "0  [(72, 34.8, 5, 0.0), (42, 9.8, 10, 0.0), (11, ...  \n",
       "1          [(51, 42.4, 40, 0.0), (14, 18.6, 9, 0.0)]  \n",
       "2  [(65, 16.8, 15, 0.15), (51, 42.4, 35, 0.15), (...  \n",
       "3  [(22, 16.8, 6, 0.05), (57, 15.6, 15, 0.05), (6...  \n",
       "4  [(33, 2.0, 25, 0.05), (20, 64.8, 40, 0.05), (6...  \n",
       "5  [(31, 10.0, 20, 0.0), (49, 16.0, 40, 0.0), (39...  \n",
       "6  [(55, 19.2, 21, 0.15), (24, 3.6, 15, 0.15), (7...  \n",
       "7  [(16, 13.9, 35, 0.0), (59, 44.0, 30, 0.0), (2,...  \n",
       "8         [(77, 10.4, 12, 0.0), (53, 26.2, 15, 0.0)]  \n",
       "9  [(27, 35.1, 25, 0.0), (77, 10.4, 15, 0.0), (39...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(od2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Ch03_SparkSQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
